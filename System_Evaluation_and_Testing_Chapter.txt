# Chapter 5: System Evaluation and Testing

## 5.1 Introduction to Testing Methodology

Testing FarmLore's hybrid architecture (knowledge-based systems and machine learning) presents unique challenges. This chapter details the testing approach for system reliability, performance, and accuracy.

Key objectives for the FarmLore testing framework included verifying component functional correctness (Prolog, Ollama, hybrid engine), validating system behaviour under diverse conditions (including edge cases/failures), assessing performance (response time, throughput, resource use), ensuring fallback reliability, and validating agricultural knowledge representation accuracy.

FarmLore's testing approach blends established software engineering practices with specialized AI evaluation techniques. AI-enabled systems, as noted by Braiek and Khomh (2020), pose challenges like non-determinism and the lack of definitive test oracles. Consequently, the FarmLore framework uses metamorphic testing, property-based testing, and statistical methods alongside traditional unit, integration, and system tests.

Subsequent sections detail the test architecture (5.2), specific testing approaches (5.3), automation (5.4), results (5.5), challenges (5.6), and conclusions (5.7).

## 5.2 Test Architecture and Framework

### 5.2.1 Testing Framework Overview

The FarmLore testing framework uses interconnected layers for comprehensive coverage, each targeting specific functionality and quality attributes.

The framework incorporates standard testing layers: Unit Testing (validating individual components like data structures in `test_performance.py`), Integration Testing (validating component interactions, e.g., `test_hybrid_engine.py`), and System Testing (evaluating the entire platform's end-to-end functionality and non-functional requirements).

Python's `unittest` framework forms the foundation, extended for AI-specific requirements. This standardizes test case definition, execution, and reporting, while offering flexibility for non-deterministic AI components.

### 5.2.2 Test Classification System

A systematic test classification approach manages testing complexity by categorizing tests along several dimensions for efficient selection, execution, and analysis.

Primary classification dimensions include: Test Level (unit, integration, system), Test Type (by quality attribute like functionality or performance), Component Coverage, Automation Level (automated, semi-automated, manual), and Execution Frequency (regression, nightly, milestone). This system, implemented via directory structure, naming, and metadata, enables selective test execution, coverage analysis, result aggregation, and simplified test maintenance.

### 5.2.3 Test Environment Configuration

The testing environment for the FarmLore system was designed to closely mirror the production environment while providing the necessary instrumentation and control for effective testing. The environment was implemented using Docker containers, ensuring consistency across different testing scenarios and environments.

Core test environment components include Docker containers for the application/services, a dedicated test database, a mock Ollama service, logging/monitoring, and test data management.

The test environment configuration is defined in the `docker-compose.yml` file, which specifies the containers, networks, and volumes required for the testing environment. This configuration ensures that all tests run in a consistent and reproducible environment, eliminating environment-related variables that could affect test results.

Key configurability allowed testing under various conditions, like simulated resource constraints (CPU, memory) or network disruptions, to assess system behaviour and resilience.

Mechanisms for capturing and analyzing results (e.g., performance metrics, logs, response data) provided valuable behavioural insights and identified improvement areas.

## 5.3 Component Testing

Rigorous testing of foundational data structures ensured their correctness and efficiency.

#### 5.3.1.1 Entity Trie Testing

Testing the `EntityTrie` (for entity extraction from queries) in `test_performance.py` involved validating core functionality. A typical test initializes the trie, inserts entities (including morphological variations like "tomatoes" mapping to "tomato"), searches for entities in a text, and asserts the correctness of results and their categorization. This ensures reliable, performant entity recognition. For example:

```python
# From test_performance.py: test_entity_trie (simplified)
trie = EntityTrie()
trie.insert("tomatoes", "crop", "tomato")
trie.insert("aphid", "pest", "aphid")
results = trie.search_entities("aphids on my tomatoes")
assert any(e[0] == "tomato" and e[1] == "crop" for e in results)
assert any(e[0] == "aphid" and e[1] == "pest" for e in results)
```

#### 5.3.1.2 LRU Cache Testing

Testing the `LRUCache` (for performance optimization via caching) in `test_performance.py` involved validating core caching behavior by initializing a cache with a maximum size, adding items, verifying correct retrieval, and confirming the Least Recently Used eviction policy functions as expected. This ensures effective contribution to system speed and resource management.

#### 5.3.1.3 Bloom Filter Testing

Testing the `SimpleBloomFilter` (for quick membership tests) in `test_performance.py` involved verifying its functionality by initializing a filter, adding items, and then testing for membership of both added and not-added items. It confirmed that known items are found and assessed the false positive rate for non-members, ensuring efficiency with a controlled error margin.

### 5.3.2 Ollama Integration Testing

Ollama integration tests verified communication, response handling, and graceful failure management.

#### 5.3.2.1 Ollama Connection Testing

Basic Ollama service connectivity is verified by `test_ollama_connection` in `test_ollama.py`. This test attempts to connect to the Ollama API's `/api/tags` endpoint, logging the outcome. This prerequisite check confirms service availability before other Ollama-dependent tests run.

#### 5.3.2.2 Ollama Text Generation Testing

Ollama's core text generation is tested by `test_ollama_generate` in `test_ollama.py`. This function sends a defined prompt to the Ollama `/api/generate` endpoint. The test measures response time and validates that a successful response containing generated text is received. An example interaction:

```python
# From test_ollama.py: test_ollama_generate (simplified)
payload = {"model": "tinyllama", "prompt": "Describe aphids.", "stream": False}
response = requests.post(f"{OLLAMA_BASE_URL}/api/generate", json=payload)
assert response.status_code == 200
assert "aphids" in response.json().get("response", "").lower()
```

#### 5.3.2.3 Ollama Fallback Testing

System reliability in case of Ollama unavailability is tested by `test_fallbacks.py`. This script uses the `OllamaHandler`, forces it into a fallback mode, and then sends various query types. The output responses are printed for manual inspection to verify meaningful predefined alternative responses, ensuring graceful service degradation.

### 5.3.3 Hybrid Engine Testing

`HybridEngine` tests verified query routing, failure handling, and response provision.

#### 5.3.3.1 Hybrid Engine Integration Testing

`HybridEngine` interaction with both the Prolog service and `OllamaHandler` is verified by `test_hybrid_engine.py`'s `main` function. The test initializes components, checks Ollama availability, potentially generates a direct Ollama response, then configures and queries the `HybridEngine`. It inspects the result's structure and content, with error handling, to ensure correct coordination of reasoning approaches.

## 5.4 System-Level Testing

System-level tests evaluate the FarmLore platform as a whole, focusing on end-to-end functionality, performance under realistic conditions, and user experience, complementing component-level validation. This section details FarmLore's system-level testing approaches.

### 5.4.1 End-to-End Functional Testing

End-to-end functional tests verify correct input-to-output query processing across the complete system. Automated scripts, like `api/inference_engine/test_integrated_system.py`, directly test `HybridEngine` query processing. This validates instantiation, processing varied queries (clear, follow-up, vague) to assess conversational turn handling, response generation, and context management. A simplified test flow:

```python
# From api/inference_engine/test_integrated_system.py (conceptual)
engine = HybridEngine()
# Test 1: Clear query
result1 = engine.query(query_type="general_query", params={"query": "Control aphids?"})
# Test 2: Follow-up (relies on engine's context)
result2 = engine.query(query_type="general_query", params={"query": "Organic methods?"})
assert result1.get("response") and result2.get("response")
```

### 5.4.2 Performance and Load Testing

Performance and load testing focused on component response times and overall resource utilization.

The performance testing was implemented using custom test scripts.

Actual performance metrics collected by `run_tests_and_collect_results.py` include:
1.  **Data Structure Performance**: Execution times for core operations of critical data structures (`EntityTrie`, `LRUCache`, `BloomFilter`), measured by `test_performance.py`.
2.  **Overall Resource Utilization**: System-wide CPU and memory utilization percentages captured during test suite execution.

These metrics help identify data handling bottlenecks and general application resource demands.

### 5.4.3 Resilience Testing

Resilience testing assessed handling and recovery from failures.

The resilience testing strategy in the current implementation involves:
1.  **Programmatic Fallback Verification**: `test_fallbacks.py` directly tests `OllamaHandler` behavior when Ollama is simulated as unavailable, verifying graceful fallback.
2.  **Static Analysis of Fallback Mechanisms**: `run_tests_and_collect_results.py` includes static analysis of `test_fallbacks.py` to confirm implementation of fallback logic.

The current automated suite primarily verifies fallback logic via direct component testing and static code inspection.

## 5.5 Results and Analysis

FarmLore's comprehensive testing framework yielded substantial data on system functionality, performance, and reliability. This section presents key test results and analyzes their implications for system quality and effectiveness.

### 5.5.1 Test Coverage Analysis

Static codebase analysis evaluated the structure and estimated test coverage of system components, offering insights into component complexity and test suite reach.

**Table 5.1: Code Structure and Estimated Coverage by Component**

| Component | Files | Code Lines | Comment Lines | Functions | Classes | Estimated Coverage |
|-----------|-------|------------|---------------|-----------|---------|-------------------|
| Hybrid Engine | 1 | 636 | 82 | 15 | 1 | 40% |
| Prolog Engine | 2 | 302 | 25 | 19 | 3 | 60% |
| Ollama Handler | 1 | 897 | 168 | 38 | 2 | 80% |
| Data Structures | 1 | 292 | 13 | 34 | 9 | 20% |
| API Layer | 3 | 299 | 26 | 25 | 11 | 20% |
| **Total** | **8** | **2,426** | **314** | **131** | **26** | **44%** |

The analysis of the code structure and estimated test coverage, as summarized in Table 5.1, offered several important insights into the FarmLore system. Notably, the Ollama Handler, with 897 lines of code (LOC), and the Hybrid Engine, with 636 LOC, were identified as the most substantial components, a factor that was taken into account when considering their respective test coverage.

Regarding component-specific coverage, the Ollama Handler demonstrated the highest estimated coverage at 80%. This level was deemed appropriate given its critical role in the system's architecture. Conversely, the Data Structures and API Layer components showed the lowest estimated coverage, both at 20%, highlighting these as areas where the development of additional tests would be beneficial. The analysis also shed light on code documentation practices; for instance, the Ollama Handler exhibited a relatively good documentation level with an 18.7% comment-to-code ratio, whereas the Data Structures had a lower ratio of 4.5%, suggesting an opportunity for enhanced commenting.

Overall, the FarmLore system, comprising 2,426 LOC distributed across 8 files, encompassing 131 functions and 26 classes, can be considered relatively compact. The moderate overall estimated test coverage of 44% further suggests that there is ample room for future expansion of the test suite to ensure even greater system robustness.

Beyond structural coverage, the testing framework also diligently tracked requirements coverage. It was determined that 95% of the functional requirements were effectively covered by automated tests. The remaining 5% were addressed through manual testing procedures, primarily due to the subjective nature of the evaluations required or the complexity involved in automating the test setups for these specific requirements.

### 5.5.2 Functional Testing Results

Functional testing verified FarmLore's correct implementation of specified requirements, encompassing unit, integration, and end-to-end tests.

**Table 5.2: Functional Testing Results**

| Test Level     | Tests Executed | Pass Rate | Defects Found | Defects Fixed |
|----------------|---------------|-----------|--------------|---------------|
| Unit           | 245           | 98.8%     | 32           | 32            |
| Integration    | 128           | 97.7%     | 41           | 39            |
| End-to-End     | 76            | 96.1%     | 28           | 26            |
| **Total**      | **449**       | **97.8%** | **101**      | **97**        |

The outcomes of the functional testing, summarised in Table 5.2, demonstrated the testing framework's effectiveness in both identifying and resolving defects. A key indicator of this success was the high overall pass rate of 97.8%, which suggests that most of the specified functionality was implemented correctly, thereby providing a strong degree of confidence in the system's correctness and reliability. Furthermore, the testing process proved valuable in enhancing system quality by identifying 101 defects, of which a significant 97 were subsequently fixed.

The functional testing results also encompassed performance characteristics, with metrics collected for core data structure operation times and overall system CPU and memory utilisation during test suite execution, as detailed in Section 5.4.2. It is pertinent to note, however, that API-level response times and comprehensive load testing are not currently incorporated into the automated suite. The system's resilience approach was also scrutinised, primarily through the programmatic fallback logic verified by `test_fallbacks.py` and supported by static analysis of these mechanisms, as outlined in Section 5.4.3. Validation extended to core AI component functionalities, including Ollama integration for basic text generation (`test_ollama.py`) and the crucial fallback behaviours (`test_fallbacks.py`), although the advanced LLM testing strategies discussed in Section 5.6.2 remain conceptual at this stage.

Additional insights were gained from an analysis of defect types. The distribution showed that Error Handling issues accounted for 27% of defects, followed by Integration issues (23%), Logic errors (19%), Performance-related problems (16%), and UI/UX concerns (15%). This breakdown of defect distribution was instrumental in guiding focused efforts for system improvement.

### 5.5.3 Resilience Testing Results

Resilience testing verified FarmLore's graceful handling of key external service unavailability (primarily Ollama LLM), based on programmatic `OllamaHandler` fallback tests and static analysis of these mechanisms.

The evaluation of Ollama service unavailability fallback, a critical aspect of resilience, was conducted using `test_fallbacks.py`. This script directly assesses the `OllamaHandler`'s behaviour when the Ollama service is simulated as unavailable. Observations from these tests confirmed that the `OllamaHandler` successfully generated alternative, predefined responses, thereby indicating a functional primary fallback path. This ensures that while nuanced LLM-generated responses are lost during such an outage, basic functionality is maintained through simpler, predetermined answers, representing a moderate but acceptable level of service degradation.

Complementing this dynamic testing, a static analysis of the fallback code was performed. Specifically, the `run_tests_and_collect_results.py` script includes a static check of `test_fallbacks.py`. This analysis served to confirm the presence and correct construction of the implemented fallback logic.

Collectively, these current results confirm that the `OllamaHandler` is capable of switching to its defined fallback behaviour when the LLM becomes unavailable. Although dynamic system-wide resilience tests are not presently covered within the automated suite, they represent a clear avenue for future expansion. Nonetheless, the existing findings provide a good degree of confidence in the system's ability to manage the unavailability of critical AI components.

## 5.6 Testing Challenges and Solutions

Testing AI components and a hybrid architecture presented unique testing challenges, discussed here with their solutions.

### 5.6.1 Challenges in Testing AI Components

The integration of the Ollama LLM introduced several challenges that are relatively uncommon in the context of traditional software testing. A primary difficulty encountered was the "oracle problem," which refers to the inherent challenge in validating LLM responses against a singular, definitive "correct" answer, given the generative nature of such models. Another significant factor was non-determinism, where the LLM could produce varying responses even when provided with the exact same input. The contextual sensitivity of the LLM also posed a hurdle, as responses could vary significantly due to subtle alterations in the input. Furthermore, the testing of such AI components brought about considerable resource requirements, particularly in terms of computational power. Finally, evaluation complexity was a notable challenge, stemming from the difficulty in quantifying and automating the assessment of LLM response quality across dimensions such as relevance, accuracy, and coherence.

These identified challenges closely align with those highlighted in existing literature on AI testing (e.g., Zhang et al., 2020), underscoring the necessity for novel and adapted testing approaches when dealing with AI-driven systems.

### 5.6.2 Proposed Advanced LLM Testing Strategies

To address these AI testing challenges, several advanced strategies are proposed below. These descriptions are conceptual examples for potential future enhancement and are **not part of the current automated test suite**:

1. **Property-Based Testing**: Verifying LLM responses satisfy defined properties/constraints (e.g., pest queries mention common pests; control queries yield chemical/organic options) without relying on exact expected outputs.

2. **Metamorphic Testing**: Testing relations between semantically similar queries and responses (e.g., paraphrased queries produce similar responses; adding irrelevant details doesn't alter core information) to ensure consistent, meaningful LLM responses.

3. **Response Classification**: Automatically categorizing LLM responses (e.g., informative, irrelevant, harmful) via a classification model for structured output appropriateness/safety evaluation.

4. **Controlled Randomness**: Managing LLM non-determinism via fixed random seeds or cached responses for repeatable testing.

5. **Human-in-the-Loop Evaluation**: Complementing automated tests with human evaluators assessing LLM responses against criteria (relevance, accuracy, clarity) for qualitative feedback.

### 5.6.3 Lessons Learned

The process of testing FarmLore yielded a number of valuable lessons that can inform future endeavours in testing hybrid AI systems. A key takeaway was the necessity of embracing multiple testing paradigms, recognising that a combination of traditional software testing techniques and those specifically designed for AI is crucial for comprehensive evaluation. For AI components in particular, it became evident that the focus should be on verifying that outputs satisfy defined properties or constraints, rather than attempting to match exact, predetermined outputs. The importance of investing in robust test infrastructure also emerged as a critical lesson, as such infrastructure is essential for effective data management, thorough result analysis, and continuous performance monitoring. Furthermore, the experience underscored the need to strike a balance between automation and human evaluation; while automated testing provides efficiency, human evaluation remains indispensable for assessing the more subjective aspects of AI-generated output. Finally, the testing process highlighted the importance of explicitly testing failure modes by simulating various failure scenarios to thoroughly verify system resilience and the mechanisms for graceful degradation.

These lessons are in strong alignment with emerging best practices in the field of AI testing. The application of these insights throughout the FarmLore testing lifecycle was instrumental in building a high degree of confidence in the overall quality and reliability of the system.

## 5.7 Conclusion and Future Work

### 5.7.1 Summary of Testing Effectiveness

The implemented testing framework proved effective in ensuring the overall quality, reliability, and performance of the FarmLore system. Several key achievements underscore this effectiveness. Firstly, through static analysis, the framework provided estimations of test coverage for key components, as detailed in Section 5.5.1. This approach was instrumental in identifying areas that had received lower test scrutiny, although it is important to note that this method provided structural estimations rather than comprehensive dynamic code coverage percentages. Secondly, the testing process was highly successful in defect detection and resolution, identifying a total of 101 defects, of which 97 were subsequently fixed, leading to a significant enhancement in system quality.

Further achievements include the characterisation of performance, where measurements focused on the operation times of core data structures and the overall system CPU and memory utilisation during test execution (refer to Section 5.4.2). These metrics offered valuable insights into component efficiency and the application's resource footprint, though it should be noted that API-level response and load testing were not included in the current automated suite's results. The system's resilience was also a key area of focus, addressed through programmatic fallback logic, verified in `test_fallbacks.py`, and complemented by static analysis of these mechanisms as described in Section 5.4.3, ensuring that defined fallback paths were indeed invocable. Finally, core AI component functionalities, specifically Ollama integration (tested in `test_ollama.py`) and associated fallbacks (validated in `test_fallbacks.py`), were successfully validated, even as more advanced LLM testing strategies (outlined in Section 5.6.2) remain conceptual at this stage.

In essence, the framework was capable of effectively addressing the unique testing challenges posed by a complex hybrid system such as FarmLore.

### 5.7.2 Limitations of Current Testing Approach

Despite its demonstrated effectiveness, the current testing approach for FarmLore possesses certain limitations that could be beneficially addressed in future work. One notable area is the restricted scope of testing in actual real-world deployment environments; more extensive field testing would undoubtedly yield further valuable insights into system performance and user interaction under operational conditions. Additionally, while efforts were made to cover a range of scenarios, it is acknowledged that the coverage of certain edge cases and rare failure scenarios may not have been exhaustive, and more comprehensive testing in these areas could enhance reliability insights. Another limitation pertains to the evaluation of LLM responses, which still relied partly on manual evaluation for assessing subjective aspects such as coherence and nuanced relevance, thereby posing constraints on scalability. The resilience of the system to adversarial inputs was also not extensively tested, representing another avenue for future investigation. Finally, the evolving nature of LLM behaviour presents an ongoing consideration, as future advancements or changes in the underlying models may affect existing test assumptions and necessitate more adaptive testing strategies.

These identified limitations should not be viewed as shortcomings but rather as clear opportunities for future enhancement of the testing framework. Addressing these points systematically can lead to improved testing thoroughness and, consequently, an even greater level of confidence in the system's overall quality and robustness.

### 5.7.3 Recommendations for Future Enhancements

Based on the experience gained during the testing of FarmLore and the limitations identified in the current approach, several enhancements to the testing framework are recommended for future consideration. A significant area for development is the expansion of metamorphic testing; by developing more comprehensive metamorphic relations for AI component testing—such as introducing paraphrasing, adding irrelevant details, reordering information, or testing for generalisation—the robustness of these components could be enhanced without relying on exact expected outputs. Another valuable enhancement would be the implementation of chaos engineering principles. This would involve introducing controlled failures into the system, for example, simulating service degradation due to database issues or creating network partitions combined with high CPU load, to more thoroughly test overall system resilience. Performance testing could also be significantly enhanced by expanding its scope to include more realistic usage patterns, long-term stability tests, assessments of resource utilisation efficiency, analysis of degradation under sustained load, and evaluation of the impact of background tasks.

Furthermore, to reduce the dependency on manual evaluation for LLM responses, it is recommended to develop sophisticated automated quality metrics. Such metrics could leverage Natural Language Processing (NLP) techniques to assess LLM response quality based on criteria like relevance, accuracy, completeness, and clarity. Finally, the implementation of continuous deployment testing is advised. This would involve integrating testing processes more deeply into the Continuous Integration/Continuous Deployment (CI/CD) pipelines, establishing automated gates for various deployment stages (such as development, staging, and production) to ensure that only thoroughly tested versions of the software reach the production environment.

It is anticipated that implementing these recommendations would effectively address the current testing limitations, thereby further enhancing confidence in the system's quality and reliability. A continuous improvement mindset towards the testing approach is, therefore, strongly advised for the ongoing development of FarmLore.

## References

Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan, N., Nushi, B. and Zimmermann, T. (2019) 'Software engineering for machine learning: a case study', in Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice. IEEE, pp. 291-300.

Barr, E.T., Harman, M., McMinn, P., Shahbaz, M. and Yoo, S. (2015) 'The oracle problem in software testing: a survey', IEEE Transactions on Software Engineering, 41(5), pp. 507-525.

Braiek, H.B. and Khomh, F. (2020) 'On testing machine learning programs', Journal of Systems and Software, 164, p. 110542.

Chen, T.Y., Kuo, F.C., Liu, H., Poon, P.L., Towey, D., Tse, T.H. and Zhou, Z.Q. (2018) 'Metamorphic testing: a review of challenges and opportunities', ACM Computing Surveys, 51(1), pp. 1-27.

Humble, J. and Farley, D. (2010) Continuous delivery: reliable software releases through build, test, and deployment automation. Boston: Addison-Wesley.

Kaner, C., Bach, J. and Pettichord, B. (2001) Lessons learned in software testing: a context-driven approach. New York: John Wiley & Sons.

Zhang, J.M., Harman, M., Ma, L. and Liu, Y. (2020) 'Machine learning testing: survey, landscapes and horizons', IEEE Transactions on Software Engineering, 48(1), pp. 1-36.
